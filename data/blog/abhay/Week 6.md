---
title: "Week 6: Getting our feet wet with Computer Vision & NLP"
desc: "Transitioning from classical ML to DNNs and ready for an exciting #Deep24"
date: 01-07-2024
author: Abhay Srivatsa
tags:
  - cv
  - nlp
  - fastai
  - andrew-ng
---

## Week In Review

I finally made it to the "good part".

This week, I was introduced to many fundamental concepts of NLP and Computer Vision, which are my favorite topics within this field. 

Learning about the different research papers such as U-net, MobileNet, YOLO, etc. has only made me want to dig deeper into the papers directly. I even got to learn about transformers, which are the building blocks for the modern LLM revolution. 

It just feels really nice to start seeing the horizon.

TLDR:
- Finished part 1 of fast.ai course 
- Finished watching Andrew Ng's Deep Learning specialization, finished about 3/5 of labs
	- Done with about 3/5 of labs
- STAT 110 lectures 22-29
- Built neural network from scratch
- Worked on famous kaggles such as handwritten digit recognition and breast cancer prediction

## Progress Timeline

### December 31, 2023

- Wrapped up ML specialization review by reading all the lecture notes
- Watched lecture 5 of fast.ai and learned how to build a neural net from scratch to solve titanic classification problem
- As practice, I started working on a notebook to implement MNIST digit classification by myself using pytorch, which I had to familiarize myself.
### January 1, 2024

- Went through fast.ai lecture 6 on random forests. The interpretation power of this algorithm is truly impressive. He didn't cover gradient boosting much, so that's definitely something I intend on exploring in-depth later on.
- Build a basic neural network with Sai using numpy and a little bit of pytorch (just for the auto-differentiation). From this prototype, we plan on adding more and more features to our mini library!
- Reviewed lecture notes for courses 1 - 3 of DL specialization.
- Completed STAT 110 lecture 22 on transformations and convolutions. Although it's been nearly a week, it feels refreshing to be back! Also super cool to see jacobian determinants popping up with N-dimensional change of variables.
### January 2, 2024

- Watched STAT 110 Lecture 23 on the Beta distribution.
- Went through fast.ai lecture 7 on collaborative filtering. He did briefly mention K-fold cross validation which is something I need to look into.
- Fleshed out the rest of the important neural network functionality. I think for the more advanced features, I'll just implement them in the labs for the DL specialization.
- Started the breast cancer detection kaggle and used random forests and logistic regression. Definitely need to iterate a lot more and produce a more elaborate model.
- Completed all the labs for weeks 1 - 3 of course 1. A lot of this is overlap from the NN I created, but it's nice to review backpropagation equations

Just two more weeks of winter break! This can only mean one thing: I'm gonna LOCK IN!
### January 3, 2024

- Watched lecture 24 on the gamma distribution.
- Worked through the rest of the labs for course 1 and 2.
- Started on course 3 on Convolutional Neural Networks and finished weeks 1 and 2.

Computer vision is particularly an exciting application as I've dabbled in it in the past, but now learning it from the ground up with NNs is truly refreshing. Overall smooth day, getting one step closer to where I want to be!
### January 4, 2024

- Watched lecture 25 on order statistics.
- Finished course 4 on CNNs after watching all the videos for weeks 3 and 4. This course genuinely sparked an interest in me to further pursue Computer Vision later on by reading more research papers on the famous network architectures like YOLO. Neural style transfer also just blew my mind away, finding about how simple it is to generate art based on certain "styles". I definitely do intend on completing all the labs diligently throughout next week after I gain a rough introduction to NLP - a topic that I've been waiting so long for!
- Being a little excited about getting to start course 5, I watched the video about Recurrent Neural Networks and how forward propagation works for it.

All in all, my CLOV journey has already started to ripen and bloom now that I'm encountering such exciting real-world applications of it, such as facial verification! Can't wait to keep learning more.
### January 5, 2024

- STAT 110 Lecture 26 on conditional expectation
- Watched all course 5 videos on RNNs, Word Embeddings, LSTMs, Transformers, and more. Truly my favorite part about the specialization, and learning about the different ways attention and context is modeled fascinated me. I'm all the more excited to jump into research papers soon!
- Implemented an RNN and LSTM from scratch including with back-propagation (week 1 lab 1). In the second lab of that week, I used the RNN implementation to train a name generator for dinosaurs.

My NLP journey has just begun.
### January 6, 2024

- STAT 110 Lecture 27. Some of the derivations at the end of the lecture confused me, but there's nothing a bit of iteration can't fix.
- Fleshed out the rest of my breast cancer notebook for presentation tommorow during the CLOV session.

Not one of my proudest days, but the ball is still rolling nonetheless.
### January 7, 2024

- Fasti AI lecture 8, wrapping up CNNs and Collaborative Filtering and also part 1 of the course. To be honest, I felt this part to be underwhelming with the over-emphasis on using their library, and "not worrying about the details for practicality". However, I'm hoping the next part digs deeper into the theoretical foundations more!
- STAT 110 lecture 28 on statistical inequalities. It feels good to finally be able to keep up with a lecture all the way through. I need to get better at thinking conditionally, and becoming fluent with conditional expectation is one of my goals for iteration. Looking forward to wrap up this course by next week!
- Implemented a CNN from scratch (C3W1 Lab), and learned how its backpropagation works as well. Though, the derivations are still a mystery.
- Used Keras to prototype a model to recognize if a face is happy or not (C3W2 Lab 1). Keras is amazingly simple to use, and the automated network graph generation was also fun to use. This library looks super handy for hackathons!

Skimming through a book on meta learning for ML by Radek Osmulski re-affirmed by beliefs on the importance of practice. As we head towards the end of winter break, practicing everything I've learned will be my number one priority through labs, kaggle, and projects. Stay tuned!
## Goals for next week

Only one week before school starts! At this point, my toolbox is filled with new tools that I just familiarized myself with, but have yet to use and hone. Next week will be focused on practicing significantly more, while also reviewing past content.

- Review fast.ai notebooks 1 - 8.
- Finish all labs of Andrew Ng and review content.
- Finish STAT 110.
- Kaggle Kaggle Kaggle!


