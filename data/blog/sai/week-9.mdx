---
title: "Week 9: XGBoost theory & Advanced house price regression"
desc: "More consistent ISLR reading and exploring gradient boosting methods." 
date: 01-28-2024
author: Venkatasai Gudisa
tags:
  - kaggle
  - islr
---
## Week in Review

Most of this week was focused on daily ISLR and continual work on the Housing Dataset. Abhay and I also reviewed Boosting and the math behind it
since it seems to be a cheat code in Kaggle Competitions. And of course, we went to tamuhack.

Towards the end of the week, I recognized the need for a more rigid schedule for me to manage my time. I've been too lackadaisical with my time
and disregarding school a little too much. The need to think about my actions everyday introduces variability into my work schedule which is almost always disasterous.
The root of success is consistency while its antithesis is variability. So the best thing to do is limit the amount of thinking I need to do for my day-to-day tasks.

## Progress Report
### January 22, 2024

- ISLR 10 Pages
- Started EDA on Housing Dataset

As you can see, I didn’t do much today due to external reasons but I think this is a good time to stand still and reflect. 

Not to toot our own horns but Abhay and I have made massive strides since we set out ~2 months ago to dive head first into the fascinating world of ML and, by extension, AI. We’re definitely come a long way and I’m proud of where we’re at. But to say we’ve just scratched the surface would be an understatement. Honestly, I think we’ve just gotten off our ship of theory and reached the iceberg of practice. As we chip away, we must remember that consistency is key! We’re going to have awesome days and not-so-awesome days. But at the end of the day, are we putting in a 100% is the only question to answer

A lot of people fall off track trying to do too much then slowing down, but I think having each other to measure against will allow us to keep pushing each other. 

Some say shoot for the stars, I say focus on the next step and you’ll be with the stars in no time!
### January 23, 2024

- ISLR 10 pages
- Reviewed Decision Trees and went deeper into Gradient Boost

XGBoosting seems to be everywhere on Kaggle so we're making it a point to understand it. Housing dataset, NLP Lecture, and TAMUHack are still cooking! One step closer!
Today was mainly back to theory, but it's something I really wanted to do for a while as boosting was part of my backlog. Random forests are cool, but boosted trees are even cooler!
### January 24, 2024

- 10 pages ISLR
- Continued learning about XGBoosting and Gradient Boosting. It's a lot but that's why it's necessary!

Not everyday is a homerun but you better be sure I'm swinging everyday!
### January 25, 2024

- 10 pages of ISLR
- Reviewed XGBoost and Boost with Abhay and brainstormed some more ideas for tamuhack

Had some other things to work one but work is definitely being done!
### January 26, 2024

- Finished chapter 5 of ISLR
- Took a step back and did some more EDA on Housing Data with trying to find the 10 most correlated pairs of columns. Going to explore this further before I think about the y-val (salesprice)

Wasn't as productive as I would've liked but tamuhack tomorrow, going in with a fresh mind!
### January 27 & 28, 2024

- 10 pages ISLR each day

We went down to TAMU for their tamuhack and had a bit of fun. It was a good exercise for our persistence and web dev skills but we shall not be deterred by its outcome! Their isn’t much to learn from hackathons other than prepare better next time but we’re on Kaggle competition grind mode now. It’s easier to learn from a Kaggle competition which makes it that much more exciting. On wards!
## Changelog

Quite often, we'd learn a fresh new concept outside of ISLR that's slightly hard to grasp, and we'd "present" our understanding on a separate day within the week. 

We did this with PCA and XGBoost, so why not make it a CLOV tradition and make it a habit? 

What's more, we also do something similar with Kaggle, as you might've noticed. Each week, we try to deep dive into a single dataset/competition and present it. What's wonderful about this approach is that it's an excellent way to improve communication in the language of data. 

Of course, we plan on continuing this habit as well, regardless of whether the notebooks we choose are the same.
## Goals for next week

- Follow the new schedule and think less while acting
- NLP 2 lectures
- Space Titanic Data set
- ISLR 10 pages
